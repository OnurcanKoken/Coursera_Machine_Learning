{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "C3_W2_Lab_1_Simple_Object_Detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmANPR2jhCR6"
      },
      "source": [
        "# Simple Object Detection in Tensorflow\n",
        "\n",
        "This lab will walk you through how to use object detection models available in [Tensorflow Hub](https://www.tensorflow.org/hub). In the following sections, you will:\n",
        "\n",
        "* explore the Tensorflow Hub for object detection models\n",
        "* load the models in your workspace\n",
        "* preprocess an image for inference \n",
        "* run inference on the models and inspect the output\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DkMLuGDhCR6"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEoRKdmByrb0"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from PIL import Image\n",
        "from PIL import ImageOps\n",
        "import tempfile\n",
        "from six.moves.urllib.request import urlopen\n",
        "from six import BytesIO"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nb8MBgTOhCR6"
      },
      "source": [
        "### Download the model from Tensorflow Hub\n",
        "\n",
        "Tensorflow Hub is a repository of trained machine learning models which you can reuse in your own projects. \n",
        "- You can see the domains covered [here](https://tfhub.dev/) and its subcategories. \n",
        "- For this lab, you will want to look at the [image object detection subcategory](https://tfhub.dev/s?module-type=image-object-detection). \n",
        "- You can select a model to see more information about it and copy the URL so you can download it to your workspace. \n",
        "- We selected a [inception resnet version 2](https://tfhub.dev/google/faster_rcnn/openimages_v4/inception_resnet_v2/1)\n",
        "- You can also modify this following cell to choose the other model that we selected, [ssd mobilenet version 2](https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9pCzz4uy20U"
      },
      "source": [
        "# you can switch the commented lines here to pick the other model\n",
        "\n",
        "# inception resnet version 2\n",
        "module_handle = \"https://tfhub.dev/google/faster_rcnn/openimages_v4/inception_resnet_v2/1\"\n",
        "\n",
        "# You can choose ssd mobilenet version 2 instead and compare the results\n",
        "#module_handle = \"https://tfhub.dev/google/openimages_v4/ssd/mobilenet_v2/1\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3trj5FbhCR6"
      },
      "source": [
        "#### Load the model\n",
        "\n",
        "Next, you'll load the model specified by the `module_handle`.\n",
        "- This will take a few minutes to load the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WHkGDHfhCR6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7988e02-1d4d-491a-f92f-f71c9691b5be"
      },
      "source": [
        "model = hub.load(module_handle)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ey0FpHGhCR6"
      },
      "source": [
        "#### Choose the default signature\n",
        "\n",
        "Some models in the Tensorflow hub can be used for different tasks. So each model's documentation should show what *signature* to use when running the model. \n",
        "- If you want to see if a model has more than one signature then you can do something like `print(hub.load(module_handle).signatures.keys())`. In your case, the models you will be using only have the `default` signature so you don't have to worry about other types."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1BU7AGthCR6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45085342-e5bf-4aff-990b-352ea6123abe"
      },
      "source": [
        "# take a look at the available signatures for this particular model\n",
        "model.signatures.keys()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KeysView(_SignatureMap({'default': <ConcreteFunction pruned(images) at 0x7FC1A9A160D0>}))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfc9ax9hhCR6"
      },
      "source": [
        "Please choose the 'default' signature for your object detector.\n",
        "- For object detection models, its 'default' signature will accept a batch of image tensors and output a dictionary describing the objects detected, which is what you'll want here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzwR5zE_hCR7"
      },
      "source": [
        "detector = model.signatures['default']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wvb-3r3thCR7"
      },
      "source": [
        "### download_and_resize_image\n",
        "\n",
        "This function downloads an image specified by a given \"url\", pre-processes it, and then saves it to disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ucsxak_qhCR7"
      },
      "source": [
        "def download_and_resize_image(url, new_width=256, new_height=256):\n",
        "    '''\n",
        "    Fetches an image online, resizes it and saves it locally.\n",
        "    \n",
        "    Args:\n",
        "        url (string) -- link to the image\n",
        "        new_width (int) -- size in pixels used for resizing the width of the image\n",
        "        new_height (int) -- size in pixels used for resizing the length of the image\n",
        "        \n",
        "    Returns:\n",
        "        (string) -- path to the saved image\n",
        "    '''\n",
        "    \n",
        "    \n",
        "    # create a temporary file ending with \".jpg\"\n",
        "    _, filename = tempfile.mkstemp(suffix=\".jpg\")\n",
        "    \n",
        "    # opens the given URL\n",
        "    response = urlopen(url)\n",
        "    \n",
        "    # reads the image fetched from the URL\n",
        "    image_data = response.read()\n",
        "    \n",
        "    # puts the image data in memory buffer\n",
        "    image_data = BytesIO(image_data)\n",
        "    \n",
        "    # opens the image\n",
        "    pil_image = Image.open(image_data)\n",
        "    \n",
        "    # resizes the image. will crop if aspect ratio is different.\n",
        "    pil_image = ImageOps.fit(pil_image, (new_width, new_height), Image.ANTIALIAS)\n",
        "    \n",
        "    # converts to the RGB colorspace\n",
        "    pil_image_rgb = pil_image.convert(\"RGB\")\n",
        "    \n",
        "    # saves the image to the temporary file created earlier\n",
        "    pil_image_rgb.save(filename, format=\"JPEG\", quality=90)\n",
        "    \n",
        "    print(\"Image downloaded to %s.\" % filename)\n",
        "    \n",
        "    return filename"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7qodEJHhCR7"
      },
      "source": [
        "### Download and preprocess an image\n",
        "\n",
        "Now, using `download_and_resize_image` you can get a sample image online and save it locally. \n",
        "- We've provided a URL for you, but feel free to choose another image to run through the object detector.\n",
        "- You can use the original width and height of the image but feel free to modify it and see what results you get."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHTDalVrhCR7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efcc17c5-2101-4d4f-84d5-14709f792e8d"
      },
      "source": [
        "# You can choose a different URL that points to an image of your choice\n",
        "image_url = \"https://upload.wikimedia.org/wikipedia/commons/f/fb/20130807_dublin014.JPG\"\n",
        "\n",
        "# download the image and use the original height and width\n",
        "downloaded_image_path = download_and_resize_image(image_url, 3872, 2592)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image downloaded to /tmp/tmp_qumed72.jpg.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVNXUKMIhCR7"
      },
      "source": [
        "### run_detector\n",
        "\n",
        "This function will take in the object detection model `detector` and the path to a sample image, then use this model to detect objects and display its predicted class categories and detection boxes.\n",
        "- run_detector uses `load_image` to convert the image into a tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkkiQzKlhCR7"
      },
      "source": [
        "def load_img(path):\n",
        "    '''\n",
        "    Loads a JPEG image and converts it to a tensor.\n",
        "    \n",
        "    Args:\n",
        "        path (string) -- path to a locally saved JPEG image\n",
        "    \n",
        "    Returns:\n",
        "        (tensor) -- an image tensor\n",
        "    '''\n",
        "    \n",
        "    # read the file\n",
        "    img = tf.io.read_file(path)\n",
        "    \n",
        "    # convert to a tensor\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    \n",
        "    return img\n",
        "\n",
        "\n",
        "def run_detector(detector, path):\n",
        "    '''\n",
        "    Runs inference on a local file using an object detection model.\n",
        "    \n",
        "    Args:\n",
        "        detector (model) -- an object detection model loaded from TF Hub\n",
        "        path (string) -- path to an image saved locally\n",
        "    '''\n",
        "    \n",
        "    # load an image tensor from a local file path\n",
        "    img = load_img(path)\n",
        "\n",
        "    # add a batch dimension in front of the tensor\n",
        "    converted_img  = tf.image.convert_image_dtype(img, tf.float32)[tf.newaxis, ...]\n",
        "    \n",
        "    # run inference using the model\n",
        "    result = detector(converted_img)\n",
        "\n",
        "    # save the results in a dictionary\n",
        "    result = {key:value.numpy() for key,value in result.items()}\n",
        "\n",
        "    # print results\n",
        "    print(\"Found %d objects.\" % len(result[\"detection_scores\"]))\n",
        "\n",
        "    print(result[\"detection_scores\"])\n",
        "    print(result[\"detection_class_entities\"])\n",
        "    print(result[\"detection_boxes\"])\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSEeJSkxhCR7"
      },
      "source": [
        "### Run inference on the image\n",
        "\n",
        "You can run your detector by calling the `run_detector` function. This will print the number of objects found followed by three lists: \n",
        "\n",
        "* The detection scores of each object found (i.e. how confident the model is), \n",
        "* The classes of each object found, \n",
        "* The bounding boxes of each object\n",
        "\n",
        "You will see how to overlay this information on the original image in the next sections and in this week's assignment!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csanHvDIz4_t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47502c38-835a-427e-d425-1271d419cc99"
      },
      "source": [
        "# runs the object detection model and prints information about the objects found\n",
        "run_detector(detector, downloaded_image_path)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 100 objects.\n",
            "[0.6532174  0.61050534 0.60152537 0.5925555  0.5917772  0.5815494\n",
            " 0.5505312  0.49575564 0.474248   0.47322178 0.44066402 0.40511376\n",
            " 0.39803794 0.3940648  0.3714855  0.36156005 0.3615083  0.34689075\n",
            " 0.33362266 0.31252944 0.28877997 0.25758398 0.25748935 0.25196052\n",
            " 0.24782084 0.23412448 0.20432052 0.203246   0.17988358 0.17964794\n",
            " 0.1737459  0.16431472 0.16031112 0.1589524  0.15620296 0.1546878\n",
            " 0.14754461 0.13622634 0.12740096 0.12555654 0.12102725 0.1181301\n",
            " 0.1138733  0.11229142 0.11129206 0.09718708 0.09137247 0.08975989\n",
            " 0.08880227 0.08633618 0.08337441 0.08095139 0.07988674 0.07741478\n",
            " 0.07732106 0.07631034 0.07507843 0.07386062 0.07233316 0.07204039\n",
            " 0.07110164 0.06935585 0.06825587 0.06427846 0.06248581 0.06226391\n",
            " 0.06211136 0.05940076 0.05798772 0.05784452 0.05725601 0.05346686\n",
            " 0.0530434  0.05242864 0.0489259  0.04813061 0.04578469 0.04424178\n",
            " 0.04337565 0.04277819 0.04262641 0.041633   0.04081771 0.03975986\n",
            " 0.03945993 0.03944286 0.03863994 0.03769068 0.03759601 0.03566714\n",
            " 0.0335938  0.03333246 0.03275878 0.03231589 0.03134295 0.02978721\n",
            " 0.02858365 0.02856144 0.02822306 0.02787894]\n",
            "[b'Person' b'Person' b'Person' b'Person' b'Footwear' b'Person' b'Building'\n",
            " b'Bicycle' b'Window' b'Building' b'Person' b'Wheel' b'Building'\n",
            " b'Building' b'Person' b'Wheel' b'Building' b'Window' b'Window'\n",
            " b'Building' b'Person' b'Van' b'Person' b'Bicycle wheel' b'Person'\n",
            " b'Window' b'Window' b'Bicycle' b'Building' b'Window' b'Window' b'Man'\n",
            " b'Person' b'Person' b'Woman' b'Clothing' b'Bicycle wheel' b'Window'\n",
            " b'Person' b'Window' b'Land vehicle' b'Land vehicle' b'Clothing'\n",
            " b'Bicycle' b'Window' b'House' b'Land vehicle' b'Land vehicle' b'House'\n",
            " b'Man' b'Window' b'Clothing' b'Footwear' b'Person' b'Window' b'Man'\n",
            " b'Man' b'House' b'Person' b'Building' b'Clothing' b'Window' b'Person'\n",
            " b'Jeans' b'Man' b'Furniture' b'Person' b'Person' b'Person'\n",
            " b'Land vehicle' b'Person' b'Window' b'House' b'Woman' b'Window' b'Man'\n",
            " b'Person' b'Man' b'Clothing' b'Bicycle' b'Man' b'Person' b'Window'\n",
            " b'Person' b'Car' b'Man' b'Car' b'Chair' b'House' b'Window' b'Clothing'\n",
            " b'Tire' b'Clothing' b'Window' b'Land vehicle' b'Window' b'Man' b'Window'\n",
            " b'Bus' b'Clothing']\n",
            "[[5.1278782e-01 5.2925885e-01 6.0162258e-01 5.5207765e-01]\n",
            " [5.1963109e-01 6.0151273e-01 6.4617711e-01 6.3462675e-01]\n",
            " [5.0550711e-01 5.0044084e-01 6.0128838e-01 5.2308434e-01]\n",
            " [4.8633158e-01 4.1272956e-01 6.7882979e-01 4.5991975e-01]\n",
            " [8.1519139e-01 9.5612222e-01 8.4270298e-01 9.8714608e-01]\n",
            " [4.9540991e-01 9.2354840e-01 8.3568794e-01 9.9905145e-01]\n",
            " [1.1479238e-02 1.2222427e-02 7.3866987e-01 4.2463282e-01]\n",
            " [5.7767749e-01 3.6645338e-01 7.1277165e-01 4.8337570e-01]\n",
            " [0.0000000e+00 1.1926237e-01 2.2389613e-01 1.8393049e-01]\n",
            " [7.7412136e-02 4.1299814e-01 5.7953990e-01 5.6044620e-01]\n",
            " [5.1381814e-01 7.4803144e-01 5.9199321e-01 7.6661122e-01]\n",
            " [6.3213789e-01 3.5992548e-01 7.0387030e-01 4.1182616e-01]\n",
            " [0.0000000e+00 7.9705197e-01 6.7336857e-01 1.0000000e+00]\n",
            " [1.6023356e-02 6.8486959e-01 5.5876148e-01 8.1116796e-01]\n",
            " [5.0027698e-01 3.7696630e-01 6.3327295e-01 4.1450134e-01]\n",
            " [6.4054024e-01 4.4508940e-01 7.0298356e-01 4.8343766e-01]\n",
            " [0.0000000e+00 2.1905424e-01 6.6040093e-01 4.3326345e-01]\n",
            " [1.9307900e-03 0.0000000e+00 1.3937686e-01 2.6295694e-02]\n",
            " [2.5719770e-03 9.6666867e-01 1.5372866e-01 1.0000000e+00]\n",
            " [5.5720011e-04 1.5205486e-03 7.6521075e-01 2.6997706e-01]\n",
            " [5.0452483e-01 3.6118776e-01 6.3473177e-01 3.8534221e-01]\n",
            " [4.8340583e-01 6.1965084e-01 5.6270576e-01 6.6155612e-01]\n",
            " [4.9806732e-01 3.6457622e-01 6.6123945e-01 4.0497237e-01]\n",
            " [6.3127881e-01 3.6036417e-01 7.0415378e-01 4.1150135e-01]\n",
            " [5.2181387e-01 5.7764757e-01 5.8759993e-01 6.0071886e-01]\n",
            " [2.1956961e-01 3.4874475e-01 3.3837262e-01 3.7707540e-01]\n",
            " [1.2486308e-01 2.5091293e-01 2.7994090e-01 2.8158078e-01]\n",
            " [5.7718605e-01 3.6229670e-01 7.0702082e-01 4.4181088e-01]\n",
            " [2.5747442e-01 5.6756157e-01 5.3110290e-01 6.8772727e-01]\n",
            " [4.2063937e-02 8.7477314e-01 2.5277349e-01 9.1302884e-01]\n",
            " [1.5635101e-01 4.4340116e-01 2.2221322e-01 4.7578609e-01]\n",
            " [5.0196797e-01 9.2148685e-01 8.3640677e-01 1.0000000e+00]\n",
            " [5.2362251e-01 5.7025951e-01 5.8451939e-01 5.9158343e-01]\n",
            " [5.1324648e-01 6.7927653e-01 5.5099446e-01 6.9257998e-01]\n",
            " [5.1912022e-01 5.9998542e-01 6.4637846e-01 6.3403648e-01]\n",
            " [5.2429777e-01 9.2496204e-01 8.1077701e-01 9.9799955e-01]\n",
            " [6.3818729e-01 4.4291818e-01 7.0165384e-01 4.8409778e-01]\n",
            " [3.4219068e-02 3.5557473e-01 1.6225506e-01 3.7492117e-01]\n",
            " [4.8847684e-01 4.5349696e-01 6.2179554e-01 4.7972572e-01]\n",
            " [9.2878978e-04 3.0769905e-01 1.0653342e-01 3.3205971e-01]\n",
            " [4.8300898e-01 6.1990827e-01 5.6477517e-01 6.6069692e-01]\n",
            " [5.8219290e-01 3.6492977e-01 7.1388066e-01 4.8470786e-01]\n",
            " [5.2354771e-01 7.4919933e-01 5.8537805e-01 7.6531756e-01]\n",
            " [6.0915685e-01 4.2670590e-01 7.0516527e-01 4.8708901e-01]\n",
            " [3.5136861e-01 9.7485608e-01 5.5313063e-01 9.9887872e-01]\n",
            " [0.0000000e+00 8.1122333e-01 6.8641073e-01 9.9715143e-01]\n",
            " [5.7629758e-01 3.5746184e-01 7.0481229e-01 4.4027966e-01]\n",
            " [5.6489241e-01 3.6302310e-01 7.0865023e-01 4.1603634e-01]\n",
            " [1.0937551e-02 2.3315601e-02 7.2652310e-01 4.2174765e-01]\n",
            " [4.8468664e-01 4.1068605e-01 6.9468647e-01 4.6309283e-01]\n",
            " [8.0977730e-02 3.8471529e-01 2.0780872e-01 4.1174638e-01]\n",
            " [5.3828442e-01 6.0357374e-01 6.3477612e-01 6.3440865e-01]\n",
            " [6.2984461e-01 6.1497146e-01 6.4493346e-01 6.2538433e-01]\n",
            " [5.0275803e-01 3.8239595e-01 5.9614623e-01 4.1272232e-01]\n",
            " [0.0000000e+00 1.2452292e-02 1.4019351e-01 2.4738215e-02]\n",
            " [5.1444143e-01 7.4779153e-01 5.9198582e-01 7.6682740e-01]\n",
            " [5.0618213e-01 5.0040692e-01 6.0068130e-01 5.2331203e-01]\n",
            " [0.0000000e+00 2.1128355e-01 6.5079421e-01 4.3430078e-01]\n",
            " [4.8945150e-01 4.5439130e-01 5.7234013e-01 4.7647077e-01]\n",
            " [0.0000000e+00 7.0621598e-01 6.1699879e-01 8.6618966e-01]\n",
            " [5.0917292e-01 4.1628119e-01 6.6930449e-01 4.5959872e-01]\n",
            " [4.6517248e-03 8.0309421e-01 1.5985359e-01 8.4039706e-01]\n",
            " [5.2615100e-01 5.6835294e-01 5.7944036e-01 5.8281022e-01]\n",
            " [6.7192483e-01 9.4027770e-01 8.2127601e-01 9.8925078e-01]\n",
            " [5.0277025e-01 3.7388307e-01 6.4699155e-01 4.1297233e-01]\n",
            " [5.7424390e-01 2.6740086e-01 6.5776908e-01 3.2031852e-01]\n",
            " [4.8605677e-01 4.4450879e-01 6.2478888e-01 4.7350335e-01]\n",
            " [5.1724899e-01 7.5696921e-01 5.8851719e-01 7.7146548e-01]\n",
            " [5.2337497e-01 5.5785012e-01 5.7913953e-01 5.7354158e-01]\n",
            " [6.1246103e-01 4.2733246e-01 7.0608020e-01 4.8825186e-01]\n",
            " [5.2412409e-01 5.6155318e-01 5.7838535e-01 5.8047515e-01]\n",
            " [0.0000000e+00 2.4423178e-01 6.0775466e-02 2.9361343e-01]\n",
            " [1.4892096e-02 2.1473852e-03 7.4544203e-01 2.5979066e-01]\n",
            " [4.9323615e-01 9.2395020e-01 8.3711076e-01 9.9775505e-01]\n",
            " [8.3768480e-03 2.4216573e-01 4.9728528e-02 2.8316259e-01]\n",
            " [5.0533491e-01 3.6017528e-01 6.4356107e-01 3.9146179e-01]\n",
            " [5.1309913e-01 5.2379411e-01 6.0050434e-01 5.4296798e-01]\n",
            " [5.2042139e-01 6.0097867e-01 6.4612406e-01 6.3436639e-01]\n",
            " [5.1822490e-01 5.0339556e-01 5.9754866e-01 5.2268386e-01]\n",
            " [5.9419912e-01 3.6132798e-01 7.0546591e-01 4.1585335e-01]\n",
            " [5.1325643e-01 6.7931706e-01 5.5053395e-01 6.9248223e-01]\n",
            " [5.2230269e-01 5.3619546e-01 5.9756500e-01 5.5316323e-01]\n",
            " [4.2987636e-01 8.2870227e-01 5.8992827e-01 8.6432314e-01]\n",
            " [5.0488460e-01 3.8942704e-01 6.1508071e-01 4.1993609e-01]\n",
            " [5.2658856e-01 6.2717688e-01 5.6329978e-01 6.5372890e-01]\n",
            " [5.0130492e-01 3.6418903e-01 6.5996474e-01 4.0379328e-01]\n",
            " [5.1517123e-01 6.2410480e-01 5.6379539e-01 6.5800208e-01]\n",
            " [5.7313794e-01 2.6690266e-01 6.6616201e-01 3.1864023e-01]\n",
            " [8.3423562e-02 4.0741432e-01 5.8409238e-01 5.5852294e-01]\n",
            " [2.8819689e-01 4.7799095e-04 4.1436461e-01 3.6599573e-02]\n",
            " [4.9727285e-01 4.5529667e-01 5.8381712e-01 4.7793603e-01]\n",
            " [6.2716800e-01 3.6102405e-01 7.0599681e-01 4.0978017e-01]\n",
            " [5.1586103e-01 3.8005698e-01 5.9689385e-01 4.1175827e-01]\n",
            " [1.1809807e-02 3.0812180e-01 9.7285956e-02 3.2503897e-01]\n",
            " [5.1250178e-01 6.2365335e-01 5.6242216e-01 6.5764189e-01]\n",
            " [4.0100312e-01 8.8508892e-01 5.8128154e-01 9.3921441e-01]\n",
            " [5.1385325e-01 5.2948457e-01 6.0200971e-01 5.5236280e-01]\n",
            " [0.0000000e+00 1.0060605e-02 1.3615684e-01 3.1600721e-02]\n",
            " [4.8042634e-01 6.2042278e-01 5.6528455e-01 6.6015029e-01]\n",
            " [5.1935548e-01 3.6184040e-01 6.2499541e-01 3.8491967e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rgq-a3t8tlAT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}