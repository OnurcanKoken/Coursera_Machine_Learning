# 1 - The problem of very deep neural networks

Last week, you built your first convolutional neural network. In recent years, neural networks have become deeper, with state-of-the-art networks going from just a few layers (e.g., AlexNet) to over a hundred layers.

* The main benefit of a very deep network is that it can represent very complex functions. It can also learn features at many different levels of abstraction, from edges (at the shallower layers, closer to the input) to very complex features (at the deeper layers, closer to the output). 
* However, using a deeper network doesn't always help. A huge barrier to training them is vanishing gradients: very deep networks often have a gradient signal that goes to zero quickly, thus making gradient descent prohibitively slow. 
* More specifically, during gradient descent, as you backprop from the final layer back to the first layer, you are multiplying by the weight matrix on each step, and thus the gradient can decrease exponentially quickly to zero (or, in rare cases, grow exponentially quickly and "explode" to take very large values). 
* During training, you might therefore see the magnitude (or norm) of the gradient for the shallower layers decrease to zero very rapidly as training proceeds: 

Yani diyo ki gradienti gittikçe kayboluyo çok çok derin sinir ağlarında, vanishing.
Residual Network ile bu problemi çözüyoruz, gradientlerin çok derin ağlarda kaybolmaması sağlanıyor.

* it very easy for one of the blocks to learn an identity function. 
* helping with vanishing gradients

the "identity block" and the "convolutional block."

2.1 identity block
x -> conv2d - batch norm - relu -> conv2d - batchnorm + x -> relu -> y
    
* To speed up training we have also added a BatchNorm step.

2.2 convolutional block
x -> conv2d - batch norm - relu -> conv2d - batchnorm - relu -> conv2d - batchnorm + (x->conv2d-batchnorm->y) -> relu -> y

use this type of block when the input and output dimensions don't match up
The difference with the identity block is that there is a CONV2D layer in the shortcut path

The CONV2D layer in the shortcut path is used to resize the input x to a different dimension
so that the dimensions match up in the final addition 

* Very deep "plain" networks don't work in practice because they are hard to train due to vanishing gradients.
* The skip-connections help to address the Vanishing Gradient problem. They also make it easy for a ResNet block to learn an identity function.








